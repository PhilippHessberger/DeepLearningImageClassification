{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Import und Initialisierung"],"metadata":{"id":"uzJtMQZEz7nx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"LiMWzjWDPVpZ","executionInfo":{"status":"ok","timestamp":1685320923717,"user_tz":-120,"elapsed":28594,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0918f1c-b242-4b1b-d7fa-f8685955b7cf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import tensorflow as tf\n","import numpy as np\n","import absl.logging\n","import random"],"metadata":{"id":"SiY-yPJvpUBk","executionInfo":{"status":"ok","timestamp":1685320930994,"user_tz":-120,"elapsed":7280,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gAsyKxhFMYcx","executionInfo":{"status":"ok","timestamp":1685320930995,"user_tz":-120,"elapsed":6,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}}},"outputs":[],"source":["# so 'WARNING:absl:Found untraced functions' does not show up again\n","absl.logging.set_verbosity(absl.logging.ERROR)\n","\n","# flags for easier changing of code behavior\n","create_new_dataset = False\n","# if you want to train a model\n","training = True\n","# if you want to load a trained model\n","load_model = None # 'model_vgg19_base_dense50relu_batchnorm_dense10softmax_4epoch_10labels_alt_data'\n","# if you want to evaluate a loaded OR trained model\n","validation = False\n","# if you want to do final evaluation on a loaded OR trained model\n","testing = False\n","single_label_testing = False\n","# if you want to save a model and do a test or validation evaluation on a model\n","model_file_name = 'model_vgg19_base_dense50relu_batchnorm_dense10softmax_4epoch_10labels_benchmark_local_vs_colab'\n","# self-explanatory\n","number_of_epochs = 4\n","\n","directory_names = [\n","    'Viburnum tinus',\n","    'Crataegus monogyna',\n","    'Hedera helix',\n","    'Ulmus minor',\n","    'Arbutus unedo',\n","    'Platanus x',\n","    'Robinia pseudoacacia',\n","    'Buxus sempervirens',\n","    'Betula pendula',\n","    'Corylus avellana'\n","]\n","\n","# batch_size = 32\n","filepath = '/content/drive/MyDrive/colab_project/'\n","training_folder = 'training_data'       # 70% -> 700 per label, but everything x3 for unfiltered data\n","validation_folder = 'validation_data'   # 20% -> 200 per label\n","testing_folder = 'testing_data'         # 10% -> 100 per label\n","dataset_folder = 'dataset'\n","checkpoint_folder = 'model_checkpoints'\n","single_label_testing_folder = 'single_label_testing_folder'"]},{"cell_type":"markdown","source":["# utils"],"metadata":{"id":"PGHbYJ8A0peh"}},{"cell_type":"markdown","source":["image_augmenter"],"metadata":{"id":"NOC-9uz114nY"}},{"cell_type":"code","source":["# shapes an image and rotates it by a random amount\n","def rotate_image_random(image, shape=None):\n","    if shape is not None:\n","        image = reshape_image(image, shape)\n","    height, width = image.shape[:2]\n","    angle = random.randint(0, 360)\n","    rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n","    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n","    mask = cv2.inRange(rotated_image, (0, 0, 0), (0, 0, 0))\n","    rotated_image[mask == 255] = (255, 255, 255)\n","\n","    return rotated_image\n","\n","def change_white_background(image, background_image=None, shape=None):\n","\n","    # Convert image to HSV color space\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n","    # Define lower and upper bounds for white color\n","    lower_white = np.array([0, 0, 100])\n","    upper_white = np.array([360, 25, 255])\n","\n","    # Create a mask for white pixels\n","    mask = cv2.inRange(hsv, lower_white, upper_white)\n","\n","    # Perform closing operation on white pixels\n","    kernel = np.ones((3, 3), np.uint8)\n","    closed_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n","\n","    # Invert the mask\n","    inverted_mask = cv2.bitwise_not(closed_mask)\n","\n","    # Apply the mask to the image\n","    result = cv2.bitwise_and(image, image, mask=inverted_mask)\n","\n","    if background_image is not None:\n","        background_image = reshape_image(background_image, shape)\n","        random_color_image = background_image\n","    else:\n","        # Generate a random color in BGR format with the same shape and dtype as the image\n","        random_color_image = np.full(image.shape, np.random.randint(0, 256, size=3, dtype=np.uint8))\n","\n","    # Replace the white pixels with the random color\n","    background = cv2.bitwise_and(random_color_image, random_color_image, mask=closed_mask)\n","\n","    # Combine the result and the background\n","    final_image = cv2.add(result, background)\n","\n","    cv2.waitKey()\n","    cv2.destroyAllWindows()\n","\n","    return final_image\n","\n","def closing_operation(image, rgb_color):\n","    black_image = np.zeros(image.shape, dtype=np.uint8)\n","    # create a mask that has 1 for every pixel of the color rgb_color\n","    mask = cv2.inRange(image, rgb_color - 1, rgb_color + 1)\n","\n","    # reverse mask, so foreground is 1\n","    reversed_mask = cv2.bitwise_not(mask)\n","\n","    kernel = np.ones((3, 3), np.uint8)\n","\n","    closed_mask = cv2.morphologyEx(reversed_mask, cv2.MORPH_CLOSE, kernel)\n","\n","    result = cv2.bitwise_and(image, image, mask=closed_mask)\n","\n","    return result\n","\n","def reshape_image(image, shape):\n","    height, width = image.shape[:2]\n","\n","    ratio = min(shape[0] / width, shape[1] / height)\n","    new_size = (int(width * ratio), int(height * ratio))\n","\n","    resized_img = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n","\n","    top = ((shape[1] - new_size[1]) // 2)\n","    bottom = ((shape[1] - new_size[1]) // 2)\n","    left = ((shape[0] - new_size[0]) // 2)\n","    right = ((shape[0] - new_size[0]) // 2)\n","\n","    if (shape[1] - new_size[1]) % 2 == 1:\n","        top = top + 1\n","    if (shape[0] - new_size[0]) % 2 == 1:\n","        right = right + 1\n","\n","    result = cv2.copyMakeBorder(resized_img,\n","                                top,\n","                                bottom,\n","                                left,\n","                                right,\n","                                cv2.BORDER_CONSTANT,\n","                                value=(255, 255, 255))\n","\n","    return result"],"metadata":{"id":"7IpYJnga1xrq","executionInfo":{"status":"ok","timestamp":1685320930995,"user_tz":-120,"elapsed":5,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["image_filter"],"metadata":{"id":"wvtDrHGh2BrS"}},{"cell_type":"code","source":["def filter_by_white_background(images, filenames=None):\n","    # create an empty list to store filtered images\n","    filtered_images = []\n","    filtered_images_filenames = []\n","\n","    # loop through images\n","    # for image in images:\n","    for i in range(len(images)):\n","        # convert the image to HSV color space\n","        hsv = cv2.cvtColor(images[i], cv2.COLOR_BGR2HSV)\n","\n","        # define lower and upper bounds for white\n","        lower_white = np.array([0, 0, 100])\n","        upper_white = np.array([360, 25, 255])\n","\n","        # create mask for white color\n","        mask = cv2.inRange(hsv, lower_white, upper_white)\n","\n","        # count number of white pixels in mask\n","        white_pixels = np.sum(mask == 255)\n","\n","        # calculate percentage of white pixels in mask\n","        white_percentage = white_pixels / (mask.shape[0] * mask.shape[1])\n","\n","        # if percentage is above a certain threshold, add image to filtered list\n","        if white_percentage > 0.5:\n","            filtered_images.append(images[i])\n","            if filenames is not None:\n","                filtered_images_filenames.append(filenames[i])\n","\n","    # return filtered list\n","    if len(filtered_images_filenames) > 0 and len(filtered_images) > 0:\n","        return filtered_images, filtered_images_filenames\n","    elif len(filtered_images) > 0:\n","        return filtered_images\n","    elif filenames is None:\n","        return None\n","    else:\n","        return None, None"],"metadata":{"id":"o5L3Xdck17ia","executionInfo":{"status":"ok","timestamp":1685320930996,"user_tz":-120,"elapsed":6,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["image_handler"],"metadata":{"id":"PXYPjs1q2Mbz"}},{"cell_type":"code","source":["def load_images_from_folder(folder_path, start_index=0, amount=-1):\n","    # containers\n","    images = []\n","    filenames = []\n","\n","    # get all filenames:\n","    for filename in os.listdir(folder_path):\n","        filenames.append(filename)\n","\n","    # if wanted amount is bigger than actual amount of files existing in the directory\n","    if amount > len(filenames) or amount == -1:\n","        amount = len(filenames)\n","\n","    # remove file extension from filename\n","    filenames = sorted(filenames, key=lambda x: int(os.path.splitext(x)[0]))\n","\n","    # only load the files we want (from start_index to start_index + amount)\n","    for filename in filenames[start_index:start_index + amount]:\n","        img = cv2.imread(os.path.join(folder_path, filename))\n","        if img is not None:\n","            images.append(img)\n","\n","    return images, filenames[start_index:start_index + amount]\n","\n","# displays the given images, space to go to next image, ESC to abort\n","def display_images(images):\n","    for img in images:\n","        cv2.namedWindow('image')\n","        cv2.moveWindow('image', 50, 50)\n","        cv2.imshow('image', img)\n","        k = cv2.waitKey(0)\n","        if k == 27:         # wait for ESC key to exit display mode\n","            cv2.destroyAllWindows()\n","            break\n","        elif k == 32:       # wait for space bar to show next image\n","            continue\n","    cv2.destroyAllWindows()\n","\n","# displays just one image until a any key is pressed\n","def display_image(img, window_title=None):\n","    if window_title is not None:\n","        cv2.namedWindow(window_title)\n","        cv2.moveWindow(window_title, 50, 50)\n","        cv2.imshow(window_title, img)\n","    else:\n","        cv2.namedWindow('image')\n","        cv2.moveWindow('image', 50, 50)\n","        cv2.imshow('image', img)\n","    cv2.waitKey()\n","    cv2.destroyAllWindows()\n","\n","# used to load a dataset into a dict. structure is like this: dict{'label' : (images_for_label, filenames_for_images)}\n","def load_dataset(filepath, label_names, image_shape=(299, 299), multiplier=1, limit=None, use_originals=False, filter_white_background=False, augment_filtered_dataset=False, backgrounds_foldername=None, only_resize_and_rotate=False):\n","    if only_resize_and_rotate:\n","        augment_filtered_dataset = False\n","        filter_white_background = False\n","\n","    # dict of 'latin name' : (images, filenames)\n","    dataset = {}\n","\n","    # load every image for each wanted label\n","    for label_name in label_names:\n","        print(f'now loading: {filepath}/{label_name}')\n","        images, filenames = load_images_from_folder(f'{filepath}/{label_name}')\n","\n","        # if the background should be filled with a random image instead of a random color\n","        if backgrounds_foldername is not None:\n","            background_images, background_filenames = load_images_from_folder('D:/Repositories/Datasets/PlantCLEF/our_selected_dataset/random_backgrounds') # f'{filepath}/{backgrounds_foldername}')\n","\n","        # filter for white background if wanted\n","        if images is not None and filenames is not None:\n","\n","            if filter_white_background:\n","                filtered_images, filenames = filter_by_white_background(images, filenames)\n","\n","                # rotate image and change background color if wanted\n","                if augment_filtered_dataset:\n","\n","                    # calculate how to reach limit if wanted\n","                    if limit is not None:\n","                        amount_of_images = len(filtered_images)\n","                        amount_of_images_to_create = limit - amount_of_images\n","                        multiplier = amount_of_images_to_create // amount_of_images\n","                        rest = amount_of_images_to_create % amount_of_images\n","                    else:\n","                        rest = 0\n","\n","                    augmented_filtered_images = []\n","                    filenames_for_augmented_filtered_images = []\n","\n","                    for i, image in enumerate(filtered_images):\n","\n","                        if rest > 0:\n","                            actual_multiplier = multiplier + 1\n","                            rest -= 1\n","                        else:\n","                            actual_multiplier = multiplier\n","\n","                        if use_originals:\n","                            augmented_filtered_images.append(reshape_image(image, image_shape))\n","                            augmented_filename = filenames[i].split('.')\n","                            augmented_filename = str(augmented_filename[0]) + '_original.' + augmented_filename[\n","                                1]\n","                            filenames_for_augmented_filtered_images.append(augmented_filename)\n","\n","                        for j in range(actual_multiplier):\n","                            # changes the images dimensions (given shape) and rotates the image\n","                            augmented_image = rotate_image_random(image, image_shape)\n","\n","                            # select random image as background\n","                            \"\"\"\n","                            if backgrounds_foldername is not None:\n","                                background_image = random.choice(background_images)\n","                            else:\n","                                background_image = None\n","                            \"\"\"\n","                            # changes the background color of the image\n","                            augmented_image = change_white_background(augmented_image) # , background_image=background_image, shape=image_shape)\n","\n","                            augmented_filtered_images.append(augmented_image)\n","\n","                            # different image names are needed for every image, but we still want to know where they were derived from\n","                            augmented_filename = filenames[i].split('.')\n","                            augmented_filename = str(augmented_filename[0]) + '_v' + str(j) + '.' + augmented_filename[1]\n","                            filenames_for_augmented_filtered_images.append(augmented_filename)\n","\n","                    dataset[label_name] = (augmented_filtered_images, filenames_for_augmented_filtered_images)\n","\n","                else:\n","                    dataset[label_name] = (filtered_images, filenames)\n","\n","            elif only_resize_and_rotate:\n","                # calculate amount of images to generate until limit per label is reached\n","                if limit is not None:\n","                    amount_of_images = len(images)\n","                    amount_of_images_to_create = limit - amount_of_images\n","                    multiplier = amount_of_images_to_create // amount_of_images\n","                    rest = amount_of_images_to_create % amount_of_images\n","                else:\n","                    rest = 0\n","\n","                augmented_images = []\n","                filenames_for_augmented_images = []\n","\n","                for i, image in enumerate(images):\n","\n","                    if rest > 0:\n","                        actual_multiplier = multiplier + 1\n","                        rest -= 1\n","                    else:\n","                        actual_multiplier = multiplier\n","\n","                    if use_originals:\n","                        augmented_images.append(reshape_image(image, image_shape))\n","                        augmented_filename = filenames[i].split('.')\n","                        augmented_filename = str(augmented_filename[0]) + '_original.' + augmented_filename[1]\n","                        filenames_for_augmented_images.append(augmented_filename)\n","\n","                    for j in range(actual_multiplier):\n","                        # changes the images dimensions (given shape) and rotates the image\n","                        augmented_image = rotate_image_random(image, image_shape)\n","\n","                        augmented_images.append(augmented_image)\n","\n","                        # different image names are needed for every image, but we still want to know where they were derived from\n","                        augmented_filename = filenames[i].split('.')\n","                        augmented_filename = str(augmented_filename[0]) + '_v' + str(j) + '.' + augmented_filename[1]\n","                        filenames_for_augmented_images.append(augmented_filename)\n","\n","                dataset[label_name] = (augmented_images, filenames_for_augmented_images)\n","\n","            else:\n","                dataset[label_name] = (images, filenames)\n","        else:\n","            print('ERROR: length of list of images and length of list of images filenames are different')\n","\n","    return dataset\n","\n","# converts our dataset dict to two lists of images and filenames\n","def convert_dataset_to_arrays(old_dataset):\n","    images = []\n","    labels = []\n","\n","    for label in old_dataset:\n","        for i in range(len(old_dataset[label][0])):\n","            try:\n","                images.append(old_dataset[label][0][i])\n","                labels.append(label)\n","            except TypeError:\n","                print(f\"WARNING: label '{label}' has no images in the dataset\")\n","\n","    return images, labels\n","\n","def augment_and_save_dataset(filepath_to_data, directory_names, dataset_name, limit, random_backgrounds=None, only_resize_and_rotate=False):\n","    filepath_to_data = filepath_to_data + dataset_name\n","\n","    my_dataset = load_dataset(filepath_to_data,\n","                              directory_names,\n","                              image_shape=(299, 299),\n","                              use_originals=True,\n","                              limit=limit,\n","                              filter_white_background=True,\n","                              augment_filtered_dataset=True,\n","                              only_resize_and_rotate=only_resize_and_rotate,\n","                              backgrounds_foldername=random_backgrounds)\n","    save_dataset(my_dataset, folder_name=dataset_name)\n","\n","def load_tensorflow_dataset_from_folder(filepath=None, folder_name=None):\n","    # catch unwanted behavior\n","    if filepath is not None and folder_name is not None:\n","        raise Exception('Set only one of the two parameters')\n","    elif filepath is not None:\n","        filepath_to_data = filepath\n","    elif folder_name is not None:\n","        filepath_to_data = os.path.join(os.getcwd(), 'data', folder_name)\n","    else:\n","        raise Exception('Set one of the two parameters')\n","\n","    # load the training dataset\n","    training_dataset = tf.keras.utils.image_dataset_from_directory(filepath_to_data, labels='inferred')\n","\n","    # convert labels to one-hot encoded vectors\n","    training_dataset = training_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=10)))\n","\n","    return training_dataset\n","\n","def save_dataset(data, folder_name):\n","    # get path to 'training_data_big' folder\n","    project_dir = os.getcwd()\n","\n","    # create data folder\n","    if not os.path.exists('data'):\n","        os.mkdir('data')\n","\n","    # change current dir to 'data' folder\n","    os.chdir(os.path.join(project_dir, 'data'))\n","\n","    # create folder\n","    if not os.path.exists(folder_name):\n","        os.mkdir(folder_name)\n","\n","    # change current dir to <folder_name> folder\n","    os.chdir(os.path.join(project_dir, 'data', folder_name))\n","\n","    # iterate over classes of our training data\n","    for label in data.keys():\n","\n","        # create folder for class if it did not exist already\n","        if not os.path.exists(os.path.join(label)):\n","            os.mkdir(label)\n","\n","        # iterate over images and filenames of these images\n","        for i in range(len(data[label][0])):\n","            # save the image in it using the filename of the image\n","            cv2.imwrite(os.path.join(label, data[label][1][i]), data[label][0][i])\n","\n","    # change current dir back to project dir\n","    os.chdir(project_dir)"],"metadata":{"id":"t_TBavSS2Obb","executionInfo":{"status":"ok","timestamp":1685320930996,"user_tz":-120,"elapsed":6,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Datensatz"],"metadata":{"id":"lwe5usIjy-u6"}},{"cell_type":"code","source":["# create own dataset\n","if create_new_dataset:\n","    dataset_filepath = filepath + dataset_folder\n","    \n","    augment_and_save_dataset(dataset_filepath, directory_names, training_folder, limit=700) # , only_resize_and_rotate=True)\n","    \n","    augment_and_save_dataset(dataset_filepath, directory_names, validation_folder, limit=200) # , only_resize_and_rotate=True)\n","    \n","    augment_and_save_dataset(dataset_filepath, directory_names, testing_folder, limit=100) # , only_resize_and_rotate=True)\n"],"metadata":{"id":"45Nu9j6AjUjc","executionInfo":{"status":"ok","timestamp":1685321948823,"user_tz":-120,"elapsed":1052,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# load, filter and augment the dataset\n","image_path = filepath + training_folder\n","my_tensorflow_dataset = tf.keras.utils.image_dataset_from_directory(image_path, labels='inferred')\n","\n","# convert labels to one-hot encoded vectors\n","my_tensorflow_dataset = my_tensorflow_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=10)))"],"metadata":{"id":"Uq0Y7s_PMf6r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685319691868,"user_tz":-120,"elapsed":5758,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}},"outputId":"58d12833-beb2-48f9-8b6e-ae96c6eb7d87"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3000 files belonging to 3 classes.\n"]}]},{"cell_type":"markdown","source":["# CNN Model"],"metadata":{"id":"yDX3DaEmzHTw"}},{"cell_type":"code","source":["# load inceptionv3 model base\n","# inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n","# vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n","# vgg19.VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n","# nasnet.NASNetMobile(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n","# nasnet.NASNetLarge(weights='imagenet', include_top=False, input_shape=(256, 256, 3))  !!! ATTENTION !!! MEMORY ERROR !!!\n","\n","model_base = tf.keras.applications.vgg19.VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n","# freeze trained layers\n","for layer in model_base.layers:\n","    layer.trainable = False\n","\n","# base up until dense layers\n","z = model_base.output\n","z = tf.keras.layers.Flatten()(z)\n","\n","# add own, untrained layers\n","z = tf.keras.layers.Dense(units=50, activation='relu')(z)\n","# z = tf.keras.layers.Dense(units=100, activation='relu')(z)\n","\n","# ideas to try out:\n","# model.add(layers.BatchNormalization()) # after dense layer\n","z = tf.keras.layers.BatchNormalization()(z)\n","\n","# add output layer\n","predictions = tf.keras.layers.Dense(units=10, activation='softmax')(z)\n","model = tf.keras.models.Model(inputs=model_base.input, outputs=predictions)\n","\n","# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.1,\n","#                                                              decay_steps=100000,\n","#                                                              decay_rate=0.96,\n","#                                                              staircase=True)\n","\n","# compile with optimizer and loss\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hkpPTV7Z2ybC","executionInfo":{"status":"ok","timestamp":1685319699236,"user_tz":-120,"elapsed":1445,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}},"outputId":"f9d737ee-41fa-4020-9c9b-088ab987a55e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80134624/80134624 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["# Model trainieren"],"metadata":{"id":"pfeIfgraz14u"}},{"cell_type":"code","source":["checkpoint_filepath = filepath + '/' + checkpoint_folder + '/' + 'model1.h5'\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)\n","\n","model.fit(my_tensorflow_dataset, epochs=10, callbacks=[model_checkpoint_callback])"],"metadata":{"id":"TX6Tw5qztG3f","executionInfo":{"status":"ok","timestamp":1685320853058,"user_tz":-120,"elapsed":243,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["if training:\n","\n","    # create own callback for stats logging\n","    class TrainingStats(tf.keras.callbacks.Callback):\n","        def __init__(self, stats_filepath):\n","            super().__init__()\n","            self.stats_filepath = stats_filepath\n","\n","        def on_epoch_end(self, epoch, logs=None):\n","            if logs is None:\n","                logs = {}\n","            with open(self.stats_filepath, \"a\") as f:\n","                f.write(f\"\\nEpoch {epoch}: Loss={logs.get('loss')}, Accuracy={logs.get('accuracy')}\\n\")\n","\n","    # create callback for tensorboard\n","    tensorboard_training_log = tf.keras.callbacks.TensorBoard(log_dir='logs\\\\{}'.format(model_file_name))\n","    training_dataset = load_tensorflow_dataset_from_folder(folder_name=training_folder)\n","    model.fit(training_dataset, epochs=number_of_epochs, callbacks=[TrainingStats(\"training_stats.txt\"), tf.keras.callbacks.ModelCheckpoint(\"model.h5\")])\n","\n","if model_file_name is not None:\n","    model.save('trained_models/' + str(model_file_name))\n","\n","if load_model is not None:\n","    load_model_name = 'trained_models/' + str(load_model)\n","    model = tf.keras.models.load_model(load_model_name)\n","\n","if validation:\n","    if model_file_name is not None:\n","        model_name = model_file_name\n","    elif load_model is not None:\n","        model_name = load_model\n","\n","    # create callback for tensorboard\n","    # tensorboard_validation_log = tf.keras.callbacks.TensorBoard(log_dir='logs\\\\{}'.format(str(model_name) + 'validation'))\n","    validation_dataset = load_tensorflow_dataset_from_folder(folder_name=validation_folder)\n","\n","    loss, acc = model.evaluate(validation_dataset) #, callbacks=[tensorboard_validation_log])\n","    print('model accuracy:', acc)\n","    print('model loss:', loss)\n","\n","if testing:\n","    # create callback for tensorboard\n","    # tensorboard_testing_log = tf.keras.callbacks.TensorBoard(log_dir='logs\\\\{}'.format(str(model_file_name) + 'testing'))\n","    testing_dataset = load_tensorflow_dataset_from_folder(folder_name=testing_folder)\n","    loss, acc = model.evaluate(testing_dataset) #, callbacks=[tensorboard_testing_log])\n","    print('model accuracy:', acc)\n","    print('model loss:', loss)\n","\n","if single_label_testing:\n","    # create callback for tensorboard\n","    # tensorboard_testing_log = tf.keras.callbacks.TensorBoard(log_dir='logs\\\\{}'.format(str(model_file_name) + 'single_testing'))\n","    single_label_testing_dataset = load_tensorflow_dataset_from_folder(folder_name=single_label_testing_folder)\n","    loss, acc = model.evaluate(single_label_testing_dataset) #, callbacks=[tensorboard_testing_log])\n","    print('model accuracy:', acc)\n","    print('model loss:', loss)\n"],"metadata":{"id":"mKCgqXxXy69m","executionInfo":{"status":"aborted","timestamp":1685318444954,"user_tz":-120,"elapsed":5,"user":{"displayName":"lXetnosl","userId":"11011226804200355425"}}},"execution_count":null,"outputs":[]}]}